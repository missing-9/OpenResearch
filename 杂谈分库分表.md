随着业务的飞速增长，我们的注册用户也越来越多，单个用户表数量已经达到千万级甚至更大。由于Mysql的单表建议百万级数据存储，所以这时为了保证系统查询和运行效率，肯定会考虑到分库分表。

对于分库分表，数据的分配是个重要的问题，你需要保证数据分配在这个服务器，那么在查询时也需要到该服务器上来查询，否则会造成数据查询丢失的问题。

通常是根据用户的 ID 哈希取模得到的值然后路由到对应的存储位置，计算公式为：`hash(userId) % N`，其中N为分库或分表的个数。

例如分库数为2时，计算结果为1，则ID为1010的用户存储在编号为1对应的库中。

之后业务数量持续增长，又新增一台用户服务库，当我们根据`ID=1010`去查询数据时， 我们得到的路由值是0，最后的结果就不用说了，存在编号1上的数据我们去编号为0的库上去查询肯定是得不到查询结果的。 

为了数据可用，你需要做数据迁移，按照新的路由规则对所有用户重新分配存储地址。每次的库或表的数量改变你都需要做一次全部用户信息数据的迁移。不用想这其中的工作量是有多费时费力了。

是否有某种方法，有效解决这种分布式存储结构下动态增加或删除节点所带来的问题，能保证这种不受实例数量变化影响而准确路由到正确的实例上的算法或实现机制呢？解决这些问题，一致性哈希算法诞生了。



基本思想原理

上面说的哈希取模方法，它是针对一个点的，业务布局严重依赖于这个计算的点值结果。你结算的结果是2，那么就对应到编号为2的服务器上。这样的映射就造成了业务容错性和可扩展性极低。 

我们思考下，是否可以将这个计算结果的点值赋予范围的意义？我们知道Hash取模之后得到的是一个 int 型的整值。 

既然 hash的计算结果是 int 类型，而 java 中 int 的最小值是`-2^31`，最大值是`2^31-1`。意味着任何通过哈希取模之后的无符号值都会在 `0 ~ 2^31-1`范围之间，共`2^32`个数。那我们是否可以不对服务器的数量进行取模而是直接对`2^32`取模。这就形成了一致性哈希的基本算法思想，什么意思呢？ 

**<!--这里需要注意一点：**-->

<!--默认的 hash 方法结果是有负值的情况，因此需要我们重写hash方法，保证哈希值的非负性。-->

简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 `0 ~ 2^32-1`（即哈希值是一个32位无符号整形），整个哈希环如下： 

![img](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002756037-1214590265.png) 

整个空间圆按**顺时针**方向布局，圆环的正上方的点代表0，0点右侧的第一个点代表1。以此类推2、3、4、5、6……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1在零点中方向重合，我们把这个由`2^32`个点组成的圆环称为 **Hash环**。 

那么，一致性哈希算法与上图中的圆环有什么关系呢？仍然以之前描述的场景为例，假设我们有4台服务器，服务器0、服务器1、服务器2，服务器3，那么，在生产环境中，这4台服务器肯定有自己的 IP 地址或主机名，我们使用它们各自的 IP 地址或主机名作为关键字进行哈希计算，使用哈希后的结果对`2^32`取模，可以使用如下公式示意：

```
hash（服务器的IP地址） %  2^32
```

最后会得到一个 `[0, 2^32-1]`之间的一个无符号整形数，这个整数就代表服务器的编号。同时这个整数肯定处于`[0, 2^32-1]`之间，那么，上图中的 hash 环上必定有一个点与这个整数对应。那么这个服务器就可以映射到这个环上。

多个服务器都通过这种方式进行计算，最后都会各自映射到圆环上的某个点，这样每台机器就能确定其在哈希环上的位置，如下图所示。

![img](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002853438-1820632464.png) 

如何提高容错性和扩展性

那么用户访问，如何分配访问的服务器呢？我们根据用户的 IP 使用上面相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环 **顺时针行走**，遇到的第一台服务器就是其应该定位到的服务器。 

![img](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002844069-651038924.png) 

从上图可以看出 *用户1* 顺时针遇到的第一台服务器是 *服务器3* ，所以该用户被分配给服务器3来提供服务。同理可以看出用户2被分配给了服务器2。

**1. 新增服务器节点**

如果这时需要新增一台服务器节点，一致性哈希策略是如何应对的呢？如下图所示，我们新增了一台服务器4，通过上述一致性哈希算法计算后得出它在哈希环的位置。

[![img](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002904562-1568019350.png)](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002904562-1568019350.png)

可以发现，原来访问服务器3的用户1现在访问的对象是服务器4，用户能正常访问且服务不需要停机就可以自动切换。

**2. 删除服务器节点**

如果这时某台服务器异常宕机或者运维撤销了一台服务器，那么这时会发生什么情况呢？如下图所示，假设我们撤销了服务器2。

[![img](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002915918-282458940.png)](https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190522002915918-282458940.png)

可以看出，我们服务仍然能正常提供服务，只不过这时用户2会被分配到服务1上了而已。

通过一致性哈希的方式，我们提高了我们系统的容错性和可扩展性，分布式节点的变动不会影响整个系统的运行且不需要我们做一些人为的调整策略。



# **总结**

一致性哈希一般在分布式缓存中使用的也比较多，本篇只介绍了服务的负载均衡和分布式存储，对于分布式缓存其实原理是类似的，读者可以自己举一反三来思考下。

其实，在分布式存储和分布式缓存中，当服务节点发生变化时（新增或减少），一致性哈希算法并不能杜绝数据迁移的问题，但是可以有效避免数据的全量迁移，需要迁移的只是更改的节点和它的上游节点它们两个节点之间的那部分数据。

另外，我们都知道 hash算法 有一个避免不了的问题，就是哈希冲突。对于用户请求IP的哈希冲突，其实只是不同用户被分配到了同一台服务器上，这个没什么影响。但是如果是服务节点有哈希冲突呢？这会导致两个服务节点在哈希环上对应同一个点，其实我感觉这个问题也不大，因为一方面哈希冲突的概率比较低，另一方面我们可以通过虚拟节点也可减少这种情况。





感谢：<https://www.cnblogs.com/jajian/p/10896624.html> 